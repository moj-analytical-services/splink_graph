{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"splink_graph: Graph metrics for data linkage at scale \u00b6 splink_graph is a graph utility library for use in Apache Spark. It computes graph metrics on the outputs of data linking which are useful for: - Quality assurance of linkage results and identifying false positive links - Computing quality metrics associated with groups (clusters) of linked records - Automatically identifying possible false positive links in clusters It works with graph data structures such as the ones created from the outputs of data linking - for instance the candidate pair results produced by Calculations are performed per cluster/connected component/subgraph in a parallel manner thanks to the underlying help from pyArrow . TL&DR : \u00b6 Graph Database OLAP solutions are a few and far between. If you have spark data in a format that can be represented as a network/graph then with this package: Graph-theoretic metrics can be obtained efficiently using an already existing spark infrastucture without the need for a graph OLAP solution The results can be used as is for finding the needle (of interesting subgraphs) in the haystack (whole set of subgraphs) Or one can augment the available graph-compatible data as part of preprocessing step before the data-ingestion phase in an OLTP graph database (such as AWS Neptune etc) Another use is to provide support for feature engineering from the subgraphs/clusters for supervised and unsupervised ML downstream uses. How to Install : \u00b6 For dependencies and other important technical info so you can run these functions without an issue please consult INSTALL.md on this repo Functionality offered : \u00b6 For a primer on the terminology used please look at TERMINOLOGY.md file in this repo Cluster metrics \u00b6 Cluster metrics usually have as an input a spark edgelist dataframe that also includes the component_id (cluster_id) where the edge is in. The output is a row of one or more metrics per cluster Cluster metrics currently offered: diameter (largest shortest distance between nodes in a cluster) transitivity (or Global Clustering Coefficient in the related literature) cluster triangle clustering coeff (or Local Clustering Coefficient in the related literature) cluster square clustering coeff (useful for bipartite networks) cluster node connectivity cluster edge connectivity cluster efficiency cluster modularity cluster assortativity cluster avg edge betweenness cluster weisfeiler lehman graphhash (in order to quickly test for graph isomorphisms) Cluster metrics are really helpful at finding the needles (of for example clusters with possible linking errors) in the haystack (whole set of clusters after the data linking process). Node metrics \u00b6 Node metrics have as an input a spark edgelist dataframe that also includes the component_id (cluster_id) where the edge belongs. The output is a row of one or more metrics per node Node metrics curretnly offered: Eigenvector Centrality Harmonic centrality Edge metrics \u00b6 Edge metrics have as an input a spark edgelist dataframe that also includes the component_id (cluster_id) where the edge belongs. The output is a row of one or more metrics per edge Edge metrics curretnly offered: Edge Betweeness Bridge Edges Functionality coming soon \u00b6 [x] release for MVP to be used on AWS glue and demos [x] cluster modularity based on partitions created by edge-betweenness [x] cluster number of bridges metric added [x] cluster assortativity added [x] cluster modularity based on partitions created by label propagation [ ] shallow embeddings of subgraphs/clusters (WIP) [x] Add a connected components function (from the graphframes library) [x] Add a connected components function for smaller graphs (from the networkx library) so its easier to get started. For upcoming functionality further down the line please consult the TODO.md file Contributing \u00b6 Feel free to contribute by Starting an issue. Forking the repository to suggest a change, and/or Want a new metric implemented? Open an issue and ask. Probably it can be.","title":"Home"},{"location":"index.html#splink_graph-graph-metrics-for-data-linkage-at-scale","text":"splink_graph is a graph utility library for use in Apache Spark. It computes graph metrics on the outputs of data linking which are useful for: - Quality assurance of linkage results and identifying false positive links - Computing quality metrics associated with groups (clusters) of linked records - Automatically identifying possible false positive links in clusters It works with graph data structures such as the ones created from the outputs of data linking - for instance the candidate pair results produced by Calculations are performed per cluster/connected component/subgraph in a parallel manner thanks to the underlying help from pyArrow .","title":"splink_graph: Graph metrics for data linkage at scale"},{"location":"index.html#tldr","text":"Graph Database OLAP solutions are a few and far between. If you have spark data in a format that can be represented as a network/graph then with this package: Graph-theoretic metrics can be obtained efficiently using an already existing spark infrastucture without the need for a graph OLAP solution The results can be used as is for finding the needle (of interesting subgraphs) in the haystack (whole set of subgraphs) Or one can augment the available graph-compatible data as part of preprocessing step before the data-ingestion phase in an OLTP graph database (such as AWS Neptune etc) Another use is to provide support for feature engineering from the subgraphs/clusters for supervised and unsupervised ML downstream uses.","title":"TL&amp;DR :"},{"location":"index.html#how-to-install","text":"For dependencies and other important technical info so you can run these functions without an issue please consult INSTALL.md on this repo","title":"How to Install :"},{"location":"index.html#functionality-offered","text":"For a primer on the terminology used please look at TERMINOLOGY.md file in this repo","title":"Functionality offered :"},{"location":"index.html#cluster-metrics","text":"Cluster metrics usually have as an input a spark edgelist dataframe that also includes the component_id (cluster_id) where the edge is in. The output is a row of one or more metrics per cluster Cluster metrics currently offered: diameter (largest shortest distance between nodes in a cluster) transitivity (or Global Clustering Coefficient in the related literature) cluster triangle clustering coeff (or Local Clustering Coefficient in the related literature) cluster square clustering coeff (useful for bipartite networks) cluster node connectivity cluster edge connectivity cluster efficiency cluster modularity cluster assortativity cluster avg edge betweenness cluster weisfeiler lehman graphhash (in order to quickly test for graph isomorphisms) Cluster metrics are really helpful at finding the needles (of for example clusters with possible linking errors) in the haystack (whole set of clusters after the data linking process).","title":"Cluster metrics"},{"location":"index.html#node-metrics","text":"Node metrics have as an input a spark edgelist dataframe that also includes the component_id (cluster_id) where the edge belongs. The output is a row of one or more metrics per node Node metrics curretnly offered: Eigenvector Centrality Harmonic centrality","title":"Node metrics"},{"location":"index.html#edge-metrics","text":"Edge metrics have as an input a spark edgelist dataframe that also includes the component_id (cluster_id) where the edge belongs. The output is a row of one or more metrics per edge Edge metrics curretnly offered: Edge Betweeness Bridge Edges","title":"Edge metrics"},{"location":"index.html#functionality-coming-soon","text":"[x] release for MVP to be used on AWS glue and demos [x] cluster modularity based on partitions created by edge-betweenness [x] cluster number of bridges metric added [x] cluster assortativity added [x] cluster modularity based on partitions created by label propagation [ ] shallow embeddings of subgraphs/clusters (WIP) [x] Add a connected components function (from the graphframes library) [x] Add a connected components function for smaller graphs (from the networkx library) so its easier to get started. For upcoming functionality further down the line please consult the TODO.md file","title":"Functionality coming soon"},{"location":"index.html#contributing","text":"Feel free to contribute by Starting an issue. Forking the repository to suggest a change, and/or Want a new metric implemented? Open an issue and ask. Probably it can be.","title":"Contributing"},{"location":"INSTALL.html","text":"How to install splink_graph \u00b6 The easiest way to install splink_graph is to type pip install --upgrade splink_graph on your terminal There are some dependencies such as numpy that needs to be version \"1.19.5\" and scipy needs to be \">= 1.6.0\" But hopefully these are taken care of automatically when the package is installed. There is a more important dependency on pyarrow for Pyspark 2.4.x that is discussed below splink_graph also assumes the existance of PySpark (either 2.3.x/2.3.4 or 3.x) but this is not an enforcable dependency. Without Pyspark however splink_graph will not work. Configuration details \u00b6 Using Pandas UDFs in Python in Pyspark 2.4.x : prerequisites \u00b6 This package uses Pandas UDFs for certain functionality.Pandas UDFs are built on top of Apache Arrow and bring the best of both worlds: the ability to define low-overhead, high-performance UDFs entirely in Python. With Apache Arrow, it is possible to exchange data directly between JVM and Python driver/executors with near-zero (de)serialization cost. However there are some things to be aware of if you want to use these functions. Since Arrow 0.15.0, a change in the binary IPC format requires an environment variable to be compatible with previous versions of Arrow <= 0.14.1. This is only necessary to do for PySpark users with versions 2.3.x and 2.4.x that have manually upgraded PyArrow to 0.15.0. The following can be added to conf/spark-env.sh to use the legacy Arrow IPC format: ARROW_PRE_0_15_IPC_FORMAT=1 Another way is to put the following on spark .config .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") .config(\"spark.executorEnv.ARROW_PRE_0_15_IPC_FORMAT\", \"1\") This will instruct PyArrow >= 0.15.0 to use the legacy IPC format with the older Arrow Java that is in Spark 2.3.x and 2.4.x. Not setting this environment variable will lead to a similar error as described in SPARK-29367 when running pandas_udfs or toPandas() with Arrow enabled. So all in all : either PyArrow needs to be at most in version 0.14.1 or if that cannot happen the above settings need to be be active. problems installing splink_graph as during installation gensim requirement install fails \u00b6 Please install the following before trying to install splink_graph ,and see this issue here pip install gensim==3.8.3 or pip install gensim==4.0.1 if you need the latest version of gensim. For the time being gensim 4.1.0 doesnt work in some configurations Using Pandas UDFs in Python in Pyspark 3.x \u00b6 No need for any special configuration. Testing \u00b6 In order to test splink_graph if you have cloned splink_graph please run pytest -v on a terminal while located at the root folder of the repo.","title":"How to install splink_graph"},{"location":"INSTALL.html#how-to-install-splink_graph","text":"The easiest way to install splink_graph is to type pip install --upgrade splink_graph on your terminal There are some dependencies such as numpy that needs to be version \"1.19.5\" and scipy needs to be \">= 1.6.0\" But hopefully these are taken care of automatically when the package is installed. There is a more important dependency on pyarrow for Pyspark 2.4.x that is discussed below splink_graph also assumes the existance of PySpark (either 2.3.x/2.3.4 or 3.x) but this is not an enforcable dependency. Without Pyspark however splink_graph will not work.","title":"How to install splink_graph"},{"location":"INSTALL.html#configuration-details","text":"","title":"Configuration details"},{"location":"INSTALL.html#using-pandas-udfs-in-python-in-pyspark-24x-prerequisites","text":"This package uses Pandas UDFs for certain functionality.Pandas UDFs are built on top of Apache Arrow and bring the best of both worlds: the ability to define low-overhead, high-performance UDFs entirely in Python. With Apache Arrow, it is possible to exchange data directly between JVM and Python driver/executors with near-zero (de)serialization cost. However there are some things to be aware of if you want to use these functions. Since Arrow 0.15.0, a change in the binary IPC format requires an environment variable to be compatible with previous versions of Arrow <= 0.14.1. This is only necessary to do for PySpark users with versions 2.3.x and 2.4.x that have manually upgraded PyArrow to 0.15.0. The following can be added to conf/spark-env.sh to use the legacy Arrow IPC format: ARROW_PRE_0_15_IPC_FORMAT=1 Another way is to put the following on spark .config .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") .config(\"spark.executorEnv.ARROW_PRE_0_15_IPC_FORMAT\", \"1\") This will instruct PyArrow >= 0.15.0 to use the legacy IPC format with the older Arrow Java that is in Spark 2.3.x and 2.4.x. Not setting this environment variable will lead to a similar error as described in SPARK-29367 when running pandas_udfs or toPandas() with Arrow enabled. So all in all : either PyArrow needs to be at most in version 0.14.1 or if that cannot happen the above settings need to be be active.","title":"Using Pandas UDFs in Python in Pyspark 2.4.x : prerequisites"},{"location":"INSTALL.html#problems-installing-splink_graph-as-during-installation-gensim-requirement-install-fails","text":"Please install the following before trying to install splink_graph ,and see this issue here pip install gensim==3.8.3 or pip install gensim==4.0.1 if you need the latest version of gensim. For the time being gensim 4.1.0 doesnt work in some configurations","title":"problems installing splink_graph as during installation gensim requirement install fails"},{"location":"INSTALL.html#using-pandas-udfs-in-python-in-pyspark-3x","text":"No need for any special configuration.","title":"Using Pandas UDFs in Python in Pyspark 3.x"},{"location":"INSTALL.html#testing","text":"In order to test splink_graph if you have cloned splink_graph please run pytest -v on a terminal while located at the root folder of the repo.","title":"Testing"},{"location":"TODO.html","text":"Functionality coming soon \u00b6 [ ] cluster modularity based on partitions created by spectral cut [x] cluster modularity based on partitions created by label propagation [ ] Node2Vec node embedding [ ] Edge2Vec edge embedding [ ] Feather node embedding framework [ ] FeatherG graph embedding framework [ ] more node-embeddings of subgraphs/clusters [ ] more full subgraph embeddings","title":"TODO"},{"location":"TODO.html#functionality-coming-soon","text":"[ ] cluster modularity based on partitions created by spectral cut [x] cluster modularity based on partitions created by label propagation [ ] Node2Vec node embedding [ ] Edge2Vec edge embedding [ ] Feather node embedding framework [ ] FeatherG graph embedding framework [ ] more node-embeddings of subgraphs/clusters [ ] more full subgraph embeddings","title":"Functionality coming soon"},{"location":"splink_graph_guide.html","text":"# Splink Graph Guide This guide aims to act as both a user guide and terminology guide, to aid those wanting to use Splink-Graph to understand the theory behind it as well as practical knowledge on how to use it to interpret cluster metrics. Contents: \u00b6 Graph Theory Terminology Installation guide Set-up Guide Connected Components Metrics Cluster Metrics Node Metrics Edge Metrics Resources References Graph Theory \u00b6 A language for networks/graphs. In summary, graph theory gives us a language for networks/graphs. It allows us to define graphs exactly and to quantify graph properties at all different levels. For more information on Graph Theory, Network Science by Albert-L\u00e1szl\u00f3 Barab\u00e1si is a great resource. Terminology \u00b6 Like any discipline, graphs come with their own set of nomenclature. The following descriptions are intentionally simplified\u2014more mathematically rigorous definitions can be found in any graph theory textbook (e.g. Network Science ). Graph A data structure G = (V, E) where V and E are a set of vertices/nodes and edges. Vertex/Node Represents a single entity such as a person or an object, Edge Represents a relationship between two vertices (e.g., are these two vertices/nodes friends on a social network?). Directed Graph vs. Undirected Graph Denotes whether the relationship represented by edges is symmetric or not Weighted vs Unweighted Graph In weighted graphs edges have a weight that could represent cost of traversing or a similarity score or a distance score In unweighted graphs edges have no weight and simply show connections . example: course prerequisites Subgraph A set of vertices and edges that are a subset of the full graph's vertices and edges. Degree A vertex/node measurement quantifying the number of connected edges Connected Component A strongly connected subgraph, meaning that every vertex can reach the other vertices in the subgraph. Modularity A function investigating quality that explores whether there is a true division of a cluster into subclusters or not. A positive value would suggest the existence of a sub-cluster structure, with a greater value providing more evidence. Oppositely, no change in this value suggests no underlying sub-cluster structure, or that by dividing the cluster, things are \"getting worse respectively\" ( Chatzoglou, Manassis, et al (2016) ). Shortest Path unweighted: The lowest number of edges required to traverse between two specific vertices/nodes unweighted weighted: The edges with the least traversal cost between two specific vertices/nodes. Installation Guide \u00b6 Make sure you are running this on a spark system/cluster system! Firstly, make sure you have these packages installed: * PyArrow ~ Version 0.15.0 or above * pandas ~ Version 1.1.5 or above * scipy ~ Version 1.5.4 or above * numpy ~ Version 1.19.5 or above To check use, pip show _packagename_ Then, the easiest way to install splink_graph is: pip install --upgrade splink_graph For information regarding how to install splink_graph and its dependencies, please see INSTALL.md For a ONS-DAP specific guide, please contact @EKenning Set-up Guide \u00b6 We recommend utilising Jupyter Notebooks when working with splink_graph, as this is easy to use and produces good visualisations. Once installed, you will need to install all the packages mentioned previously into your session. E.g., import splink_graph Then you will want to locate the utils folder from splink_graph.utils import _create_spark_jars_string _create spark_jars_string() And then use the output above to create a jar_string, e.g.: jar_string = '/abc/def/ghi/ You then must define your spark session. Then you are able to read in the dataframe you wish to use, or alternatively, explore the test dataframe included in the package: test_df = spark.read.parquet('df_e.snappy.parquet') Connected Components \u00b6 Once you have set up the package and read in the dataframe you wish to use, you must run the connected components function. This function will provide the edges with scores as well as a cluster id, which are vital for producing the cluster metrics. Which connected components function you choose to use, is based on the size of your dataset. For a dataset with <2 million records, nx_connected_components will need to be used. For dataset with >2 milion records, graphframes_connected_components will need to be used. For the test dataframe, nx_connected_components should be used. from graphframes import GraphFrame import networkx as nx from splink_graph.cc import nx_connected_components from splink_graph.cc import graphframes_connected_components To learn more about connected_components, use the help() function. help(graphframes_connected_components) help(nx_connected_components) You may need to rename your variables before you run connected components, particularly if using graphframes. dataframe = dataframe.withColumnRenamed('linkage_id_l', 'src')\\ .withColumnRenamed('linkage_id_r', 'dst')\\ .withColumnRenamed('match_probability', 'tf_adjusted_match_prob') You can then run your connected components function! The cc_threshold decides the threshold for a valid linkage weight. Please adjust in-line with your data. dataframe = graphframes_connected_components(dataframe, src='src', dst='dst', weight_colname='tf_adjusted_match_prob',\\ cc_threshold = 0.9) dataframe.show() Metrics \u00b6 Below are the metrics available through splink-graph, what they calculate and what they can tell you about your data. They are arranged into 3 types: Cluster Metrics cluster_basic_stats cluster_main_stats cluster_graph_hash cluster_connectivity_stats cluster_eb_modularity cluster_lpg_modularity cluster_average_edge_betweeness number_of_bridges cluster_assortativity cluster_efficiency Node Metrics eigencentrality harmoniccentrality Edge Metrics edge_betweeness bridge_edges Cluster Metrics \u00b6 cluster_basic_stats \u00b6 This provides node count, edge count, number of nodes per cluster, and density. cluster_basic_stats(df, src =\"src\", dst = \"dst\", cluster_id_colname = \"cluster_id\", weight_colname = \"weight\") Node count Number of nodes. Edge count Number of edges/vertices. Nodes per cluster Number of nodes contained within each cluster. Density \"The density of a graph is a number ranging from 0 to 1 and reflecting how connected a graph is in regards to its maximum potential connectivity. The density can also be formulated as the ratio between actual connections, i.e. the number of existing edges in the graph, and the number of maximum potential connections between vertices\" ( Croset et al (2015) ). Actual Edges / Maximum Possible Edges High density reflects a well connected graph (see above), meaning that most likely your clusters are not problematic. Low density could indicate that the cluster has the topography where there could be spurious links so it should probably be clerically reviewed . cluster_main_stats \u00b6 Calculates diameter, transivity, triangle clustering coefficient and square clustering coefficient. cluster_main_stats(sparkdf, src=\"src\", dst=\"dst\", cluster_id_colname=\"cluster_id\") Diameter The diameter of a graph is a measure of the longest distance between two nodes ( Randall et al (2014) ). I.e. the largest number of edges that must be transversed to travel from one vertex/node to another, OR the maximum length from all shortest path calculations. ( Wolfram Mathworld ) Transitivity (or Global Clustering Coefficient in the related literature) The global clustering coefficient is based on triplets of nodes in a graph. A triplet consists of three connected nodes. A triangle therefore includes three closed triplets, one centered on each of the nodes (n.b. this means the three triplets in a triangle come from overlapping selections of nodes). The global clustering coefficient is the number of closed triplets (or 3 x triangles) over the total number of triplets (both open and closed). Low transitivity is a cause for concern, especially where diameter is high but transitivity is low, and even more so if bridges are present. These clusters should be clerically reviewed. Traingle Clustering Coefficient (or Local Clustering Coefficient in the related literature) The local clustering coefficient for a node is then given by the proportion of links between the vertices within its neighborhood divided by the number of links that could possibly exist between them. I.e. the average density of the node's neighbours. The local clustering coefficient for a graph is the average LCC for all the nodes in that graph. Low LCC/TCC is cause for concern as it could suggest there are edges that link nodes that maybe shouldnt be linked. Square Clustering Coefficient Quantifies the abundance of connected squares in a graph (useful for bipartite networks where nodes cannot be connected in triangles). Like TCC/LCC, Low SCC is also cause for concern as it could suggest there are edges that link nodes that maybe shouldnt be linked. cluster_graph_hash \u00b6 Calculates Weisfeiler-Lehman graphhash of a cluster (in order to quickly test for graph isomorphisms). cluster_graph_hash(sparkdf, src=\"src\", dst=\"dst\", cluster_id_colname=\"cluster_id\") weisfeiler lehman graph hash A Weisfeiler Lehman graph hash offers a way to quickly test for graph isomorphisms. Hashes are identical for isomorphic graphs and there are strong guarantees that non-isomorphic graphs will get different hashes. Two graphs are considered isomorphic if there is a mapping between the nodes of the graphs that preserves node adjacencies. That is, a pair of nodes may be connected by an edge in the first graph if and only if the corresponding pair of nodes in the second graph is also connected by an edge in the same way. This metric is particularly helpful as when clusters present the same graph hash, you can have confidence they are the same, unlike some other metrics, where although they may look similar, they are not mathematically identical. Graph 1 and Graph 2 above are isomorphic. The correspondance between nodes is illustrated by the node colors and numbers. The version of graph hash currently held on splink_graph does take edge composition into account. cluster_connectivity_stats \u00b6 Calculates Node Connectivity and Edge Connectivity of the cluster. cluster_connectivity_stats(sparkdf, src=\"src\", dst=\"dst\", cluster_id_colname=\"cluster_id\") node connectivity Node connectivity of a graph gives for the minimum number of nodes that need to be removed to separate the remaining nodes into two or more isolated subgraphs. edge connectivity Edge connectivity of a graph gives for the minimum number of edges that need to be removed to separate the remaining nodes into two or more isolated subgraphs. The larger these values are (both node and edge connectivity), the more connected the cluster is. cluster_eb_modularity \u00b6 Caclulates the cluster edge betweeness modularity. cluster_eb_modularity(sparkdf, src=\"src\", dst=\"dst\", distance_colname = \"distance\", cluster_id_colname=\"cluster_id\") Comp eb modularity Modularity for cluster_id if it partitioned into two parts at the point where the highest edge betweeness exists. cluster_lpg_modularity \u00b6 Calculates the cluster lpg modularity. cluster_lpg_modularity(sparkdf, src=\"src\", dst=\"dst\", distance_colname = \"distance\", cluster_id_colname=\"cluster_id\") Cluster lpg modularity Modularity for cluster_id if it partitioned into 2 parts based on label propagation. cluster_avg_edge_betweeness \u00b6 Provides the average edge betweeness for each cluster id. cluster_avg_edge_betweeness(sparkdf, src=\"src\", dst=\"dst\", distance_colname = \"distance\", cluster_id_colname=\"cluster_id\") Average Edge Betweeness \u201cIn order to get a measure for the robustness of a network we can take the average of the vertex/edge betweenness. The smaller this average, the more robust the network.\" - ( Ellens and Kooij (2013) ) See below for edge betweenness definition. number_of_bridges \u00b6 Provides the number of bridges in the cluster. number_of_bridges(sparkdf, src=\"src\", dst=\"dst\", cluster_id_colname=\"cluster_id\") Number of Bridges The number of edges that join two clusters together (a bridge). A high bridge count could suggest that there are edges linking multiple clusters together when they shouldn't. This cluster should therefore be clerically reviewed. cluster_assortativity \u00b6 Calculates the assortativity of a cluster. cluster_assortativity(sparkdf, src=\"src\", dst=\"dst\", cluster_id_colname=\"cluster_id\") Cluster Assortativity \u201cAssortativity is a graph metric. It represents to what extent nodes in a network associate with other nodes in the network, being of similar sort or being of opposing sort... A network is said to be assortative when high degree nodes are, on average, connected to other nodes with high degree and low degree nodes are, on average, connected to other nodes with low degree. A network is said to be disassortative when, on average, high degree nodes are connected to nodes with low(er) degree and, on average, low degree nodes are connected to nodes with high(er) degree\" - ( Noldus and Mieghem (2015) ). This metric ranges from +1 to -1. The more negative this metric is, the more suspicious this cluster looks, and clusters -0.2 or lower should be clerically reviewed. cluster_efficiency \u00b6 code to be added Cluster Effiiency \u201cFor the efficiency it holds that the greater the value, the greater the robustness, because the reciprocals of the path lengths are used. The advantage of this measure is that it can be used for unconnected networks, such as social networks or networks subject to failures. Otherwise, it has the same disadvantage as the average path length; alternative paths are not considered\u201d - ( Ellens and Kooij (2013) ) Node Metrics \u00b6 eigencentrality \u00b6 Provides the eingenvector centrality of a cluster. eigencentrality(sparkdf, src=\"src\", dst=\"dst\", cluster_id_colname=\"cluster_id\") Eigencentrality Eigenvector Centrality is an algorithm that measures the transitive influence or connectivity of nodes. Relationships to high-scoring nodes contribute more to the score of a node than connections to low-scoring nodes. A high score means that a node is connected to other nodes that have high scores. harmoniccentrality \u00b6 Provides the harmonic centrality of a cluster. harmoniccentrality(sparkdf, src=\"src\", dst=\"dst\", cluster_id_colname=\"cluster_id\") Harmonic Centrality Harmonic centrality (also known as valued centrality) is a variant of closeness centrality, that was invented to solve the problem the original formula had when dealing with unconnected graphs. Rather than summing the distances of a node to all other nodes, the harmonic centrality algorithm sums the inverse of those distances. This enables it deal with infinite values. Edge Metrics \u00b6 edgebetweeness \u00b6 Provides the edge betweeness. edgebetweeness(sparkdf, src=\"src\", dst=\"dst\", distance_colname = \"distance\", cluster_id_colname=\"cluster_id\") Edge Betweeness \u201cThe edge betweeness graph metric counts the number of shortest paths between any two nodes from the cluster that use this edge. If there is more than one shortest path between a pair of nodes, each path is assigned equal weight such that the total weight of all of the paths is equal to unity.\" - ( Chatzoglou, Manassis, et al (2016) ). A high edge betweeness is cause for concern and so clusters may need to be clerically reviewed. bridge_edges \u00b6 Returns any edges that are bridges. bridge_edges(sparkdf, src=\"src\", dst=\"dst\", distance_colname = \"distance\", cluster_id_colname=\"cluster_id\") Bridge Edges Bridge Edges are edges which join together two distinct clusters. I.e. any edges that if removed increase the total number of clusters. When bridges are present, this increases the edge betweeness of the cluster. Resources \u00b6 For more information regarding the splink-graph package , please see MoJ github: https://github.com/moj-analytical-services/splink_graph . For a more in-depth look into graph data and graph theory , please see Network Science by Albert-L\u00e1szl\u00f3 Barab\u00e1si. References \u00b6 Chatzoglou, C., Manassis, T., Gammon, S. and Swier, N., 2016. Use of Graph Databases to Improve the Management and Quality of Linked Data. https://espace.curtin.edu.au/bitstream/handle/20.500.11937/3205/199679_199679.pdf?sequence=2&isAllowed=y Croset, S., Rupp, J. and Romacker, M., 2016. Flexible data integration and curation using a graph-based approach. Bioinformatics, 32(6), pp.918-925. https://academic.oup.com/bioinformatics/article/32/6/918/1743746 Ellens, W. and Kooij, R.E., 2013. Graph measures and network robustness. arXiv preprint arXiv:1311.5064. https://arxiv.org/pdf/1311.5064 Noldus, R. and Van Mieghem, P., 2015. Assortativity in complex networks. Journal of Complex Networks, 3(4), pp.507-542. https://nas.ewi.tudelft.nl/people/Piet/papers/JCN2015AssortativitySurveyRogier.pdf Randall, S.M., Boyd, J.H., Ferrante, A.M., Bauer, J.K. and Semmens, J.B., 2014. Use of graph theory measures to identify errors in record linkage. Computer methods and programs in biomedicine, 115(2), pp.55-63. https://espace.curtin.edu.au/bitstream/handle/20.500.11937/3205/199679_199679.pdf?sequence=2&isAllowed=y","title":"splink_graph guide"},{"location":"splink_graph_guide.html#contents","text":"Graph Theory Terminology Installation guide Set-up Guide Connected Components Metrics Cluster Metrics Node Metrics Edge Metrics Resources References","title":"Contents:"},{"location":"splink_graph_guide.html#graph-theory","text":"A language for networks/graphs. In summary, graph theory gives us a language for networks/graphs. It allows us to define graphs exactly and to quantify graph properties at all different levels. For more information on Graph Theory, Network Science by Albert-L\u00e1szl\u00f3 Barab\u00e1si is a great resource.","title":"Graph Theory  "},{"location":"splink_graph_guide.html#terminology","text":"Like any discipline, graphs come with their own set of nomenclature. The following descriptions are intentionally simplified\u2014more mathematically rigorous definitions can be found in any graph theory textbook (e.g. Network Science ). Graph A data structure G = (V, E) where V and E are a set of vertices/nodes and edges. Vertex/Node Represents a single entity such as a person or an object, Edge Represents a relationship between two vertices (e.g., are these two vertices/nodes friends on a social network?). Directed Graph vs. Undirected Graph Denotes whether the relationship represented by edges is symmetric or not Weighted vs Unweighted Graph In weighted graphs edges have a weight that could represent cost of traversing or a similarity score or a distance score In unweighted graphs edges have no weight and simply show connections . example: course prerequisites Subgraph A set of vertices and edges that are a subset of the full graph's vertices and edges. Degree A vertex/node measurement quantifying the number of connected edges Connected Component A strongly connected subgraph, meaning that every vertex can reach the other vertices in the subgraph. Modularity A function investigating quality that explores whether there is a true division of a cluster into subclusters or not. A positive value would suggest the existence of a sub-cluster structure, with a greater value providing more evidence. Oppositely, no change in this value suggests no underlying sub-cluster structure, or that by dividing the cluster, things are \"getting worse respectively\" ( Chatzoglou, Manassis, et al (2016) ). Shortest Path unweighted: The lowest number of edges required to traverse between two specific vertices/nodes unweighted weighted: The edges with the least traversal cost between two specific vertices/nodes.","title":"Terminology "},{"location":"splink_graph_guide.html#installation-guide","text":"Make sure you are running this on a spark system/cluster system! Firstly, make sure you have these packages installed: * PyArrow ~ Version 0.15.0 or above * pandas ~ Version 1.1.5 or above * scipy ~ Version 1.5.4 or above * numpy ~ Version 1.19.5 or above To check use, pip show _packagename_ Then, the easiest way to install splink_graph is: pip install --upgrade splink_graph For information regarding how to install splink_graph and its dependencies, please see INSTALL.md For a ONS-DAP specific guide, please contact @EKenning","title":"Installation Guide "},{"location":"splink_graph_guide.html#set-up-guide","text":"We recommend utilising Jupyter Notebooks when working with splink_graph, as this is easy to use and produces good visualisations. Once installed, you will need to install all the packages mentioned previously into your session. E.g., import splink_graph Then you will want to locate the utils folder from splink_graph.utils import _create_spark_jars_string _create spark_jars_string() And then use the output above to create a jar_string, e.g.: jar_string = '/abc/def/ghi/ You then must define your spark session. Then you are able to read in the dataframe you wish to use, or alternatively, explore the test dataframe included in the package: test_df = spark.read.parquet('df_e.snappy.parquet')","title":"Set-up Guide "},{"location":"splink_graph_guide.html#connected-components","text":"Once you have set up the package and read in the dataframe you wish to use, you must run the connected components function. This function will provide the edges with scores as well as a cluster id, which are vital for producing the cluster metrics. Which connected components function you choose to use, is based on the size of your dataset. For a dataset with <2 million records, nx_connected_components will need to be used. For dataset with >2 milion records, graphframes_connected_components will need to be used. For the test dataframe, nx_connected_components should be used. from graphframes import GraphFrame import networkx as nx from splink_graph.cc import nx_connected_components from splink_graph.cc import graphframes_connected_components To learn more about connected_components, use the help() function. help(graphframes_connected_components) help(nx_connected_components) You may need to rename your variables before you run connected components, particularly if using graphframes. dataframe = dataframe.withColumnRenamed('linkage_id_l', 'src')\\ .withColumnRenamed('linkage_id_r', 'dst')\\ .withColumnRenamed('match_probability', 'tf_adjusted_match_prob') You can then run your connected components function! The cc_threshold decides the threshold for a valid linkage weight. Please adjust in-line with your data. dataframe = graphframes_connected_components(dataframe, src='src', dst='dst', weight_colname='tf_adjusted_match_prob',\\ cc_threshold = 0.9) dataframe.show()","title":"Connected Components "},{"location":"splink_graph_guide.html#metrics","text":"Below are the metrics available through splink-graph, what they calculate and what they can tell you about your data. They are arranged into 3 types: Cluster Metrics cluster_basic_stats cluster_main_stats cluster_graph_hash cluster_connectivity_stats cluster_eb_modularity cluster_lpg_modularity cluster_average_edge_betweeness number_of_bridges cluster_assortativity cluster_efficiency Node Metrics eigencentrality harmoniccentrality Edge Metrics edge_betweeness bridge_edges","title":"Metrics "},{"location":"splink_graph_guide.html#cluster-metrics","text":"","title":"Cluster Metrics "},{"location":"splink_graph_guide.html#cluster_basic_stats","text":"This provides node count, edge count, number of nodes per cluster, and density. cluster_basic_stats(df, src =\"src\", dst = \"dst\", cluster_id_colname = \"cluster_id\", weight_colname = \"weight\") Node count Number of nodes. Edge count Number of edges/vertices. Nodes per cluster Number of nodes contained within each cluster. Density \"The density of a graph is a number ranging from 0 to 1 and reflecting how connected a graph is in regards to its maximum potential connectivity. The density can also be formulated as the ratio between actual connections, i.e. the number of existing edges in the graph, and the number of maximum potential connections between vertices\" ( Croset et al (2015) ). Actual Edges / Maximum Possible Edges High density reflects a well connected graph (see above), meaning that most likely your clusters are not problematic. Low density could indicate that the cluster has the topography where there could be spurious links so it should probably be clerically reviewed .","title":"cluster_basic_stats "},{"location":"splink_graph_guide.html#cluster_main_stats","text":"Calculates diameter, transivity, triangle clustering coefficient and square clustering coefficient. cluster_main_stats(sparkdf, src=\"src\", dst=\"dst\", cluster_id_colname=\"cluster_id\") Diameter The diameter of a graph is a measure of the longest distance between two nodes ( Randall et al (2014) ). I.e. the largest number of edges that must be transversed to travel from one vertex/node to another, OR the maximum length from all shortest path calculations. ( Wolfram Mathworld ) Transitivity (or Global Clustering Coefficient in the related literature) The global clustering coefficient is based on triplets of nodes in a graph. A triplet consists of three connected nodes. A triangle therefore includes three closed triplets, one centered on each of the nodes (n.b. this means the three triplets in a triangle come from overlapping selections of nodes). The global clustering coefficient is the number of closed triplets (or 3 x triangles) over the total number of triplets (both open and closed). Low transitivity is a cause for concern, especially where diameter is high but transitivity is low, and even more so if bridges are present. These clusters should be clerically reviewed. Traingle Clustering Coefficient (or Local Clustering Coefficient in the related literature) The local clustering coefficient for a node is then given by the proportion of links between the vertices within its neighborhood divided by the number of links that could possibly exist between them. I.e. the average density of the node's neighbours. The local clustering coefficient for a graph is the average LCC for all the nodes in that graph. Low LCC/TCC is cause for concern as it could suggest there are edges that link nodes that maybe shouldnt be linked. Square Clustering Coefficient Quantifies the abundance of connected squares in a graph (useful for bipartite networks where nodes cannot be connected in triangles). Like TCC/LCC, Low SCC is also cause for concern as it could suggest there are edges that link nodes that maybe shouldnt be linked.","title":"cluster_main_stats "},{"location":"splink_graph_guide.html#cluster_graph_hash","text":"Calculates Weisfeiler-Lehman graphhash of a cluster (in order to quickly test for graph isomorphisms). cluster_graph_hash(sparkdf, src=\"src\", dst=\"dst\", cluster_id_colname=\"cluster_id\") weisfeiler lehman graph hash A Weisfeiler Lehman graph hash offers a way to quickly test for graph isomorphisms. Hashes are identical for isomorphic graphs and there are strong guarantees that non-isomorphic graphs will get different hashes. Two graphs are considered isomorphic if there is a mapping between the nodes of the graphs that preserves node adjacencies. That is, a pair of nodes may be connected by an edge in the first graph if and only if the corresponding pair of nodes in the second graph is also connected by an edge in the same way. This metric is particularly helpful as when clusters present the same graph hash, you can have confidence they are the same, unlike some other metrics, where although they may look similar, they are not mathematically identical. Graph 1 and Graph 2 above are isomorphic. The correspondance between nodes is illustrated by the node colors and numbers. The version of graph hash currently held on splink_graph does take edge composition into account.","title":"cluster_graph_hash "},{"location":"splink_graph_guide.html#cluster_connectivity_stats","text":"Calculates Node Connectivity and Edge Connectivity of the cluster. cluster_connectivity_stats(sparkdf, src=\"src\", dst=\"dst\", cluster_id_colname=\"cluster_id\") node connectivity Node connectivity of a graph gives for the minimum number of nodes that need to be removed to separate the remaining nodes into two or more isolated subgraphs. edge connectivity Edge connectivity of a graph gives for the minimum number of edges that need to be removed to separate the remaining nodes into two or more isolated subgraphs. The larger these values are (both node and edge connectivity), the more connected the cluster is.","title":"cluster_connectivity_stats "},{"location":"splink_graph_guide.html#cluster_eb_modularity","text":"Caclulates the cluster edge betweeness modularity. cluster_eb_modularity(sparkdf, src=\"src\", dst=\"dst\", distance_colname = \"distance\", cluster_id_colname=\"cluster_id\") Comp eb modularity Modularity for cluster_id if it partitioned into two parts at the point where the highest edge betweeness exists.","title":"cluster_eb_modularity "},{"location":"splink_graph_guide.html#cluster_lpg_modularity","text":"Calculates the cluster lpg modularity. cluster_lpg_modularity(sparkdf, src=\"src\", dst=\"dst\", distance_colname = \"distance\", cluster_id_colname=\"cluster_id\") Cluster lpg modularity Modularity for cluster_id if it partitioned into 2 parts based on label propagation.","title":"cluster_lpg_modularity "},{"location":"splink_graph_guide.html#cluster_avg_edge_betweeness","text":"Provides the average edge betweeness for each cluster id. cluster_avg_edge_betweeness(sparkdf, src=\"src\", dst=\"dst\", distance_colname = \"distance\", cluster_id_colname=\"cluster_id\") Average Edge Betweeness \u201cIn order to get a measure for the robustness of a network we can take the average of the vertex/edge betweenness. The smaller this average, the more robust the network.\" - ( Ellens and Kooij (2013) ) See below for edge betweenness definition.","title":"cluster_avg_edge_betweeness "},{"location":"splink_graph_guide.html#number_of_bridges","text":"Provides the number of bridges in the cluster. number_of_bridges(sparkdf, src=\"src\", dst=\"dst\", cluster_id_colname=\"cluster_id\") Number of Bridges The number of edges that join two clusters together (a bridge). A high bridge count could suggest that there are edges linking multiple clusters together when they shouldn't. This cluster should therefore be clerically reviewed.","title":"number_of_bridges "},{"location":"splink_graph_guide.html#cluster_assortativity","text":"Calculates the assortativity of a cluster. cluster_assortativity(sparkdf, src=\"src\", dst=\"dst\", cluster_id_colname=\"cluster_id\") Cluster Assortativity \u201cAssortativity is a graph metric. It represents to what extent nodes in a network associate with other nodes in the network, being of similar sort or being of opposing sort... A network is said to be assortative when high degree nodes are, on average, connected to other nodes with high degree and low degree nodes are, on average, connected to other nodes with low degree. A network is said to be disassortative when, on average, high degree nodes are connected to nodes with low(er) degree and, on average, low degree nodes are connected to nodes with high(er) degree\" - ( Noldus and Mieghem (2015) ). This metric ranges from +1 to -1. The more negative this metric is, the more suspicious this cluster looks, and clusters -0.2 or lower should be clerically reviewed.","title":"cluster_assortativity "},{"location":"splink_graph_guide.html#cluster_efficiency","text":"code to be added Cluster Effiiency \u201cFor the efficiency it holds that the greater the value, the greater the robustness, because the reciprocals of the path lengths are used. The advantage of this measure is that it can be used for unconnected networks, such as social networks or networks subject to failures. Otherwise, it has the same disadvantage as the average path length; alternative paths are not considered\u201d - ( Ellens and Kooij (2013) )","title":"cluster_efficiency "},{"location":"splink_graph_guide.html#node-metrics","text":"","title":"Node Metrics "},{"location":"splink_graph_guide.html#eigencentrality","text":"Provides the eingenvector centrality of a cluster. eigencentrality(sparkdf, src=\"src\", dst=\"dst\", cluster_id_colname=\"cluster_id\") Eigencentrality Eigenvector Centrality is an algorithm that measures the transitive influence or connectivity of nodes. Relationships to high-scoring nodes contribute more to the score of a node than connections to low-scoring nodes. A high score means that a node is connected to other nodes that have high scores.","title":"eigencentrality "},{"location":"splink_graph_guide.html#harmoniccentrality","text":"Provides the harmonic centrality of a cluster. harmoniccentrality(sparkdf, src=\"src\", dst=\"dst\", cluster_id_colname=\"cluster_id\") Harmonic Centrality Harmonic centrality (also known as valued centrality) is a variant of closeness centrality, that was invented to solve the problem the original formula had when dealing with unconnected graphs. Rather than summing the distances of a node to all other nodes, the harmonic centrality algorithm sums the inverse of those distances. This enables it deal with infinite values.","title":"harmoniccentrality "},{"location":"splink_graph_guide.html#edge-metrics","text":"","title":"Edge Metrics "},{"location":"splink_graph_guide.html#edgebetweeness","text":"Provides the edge betweeness. edgebetweeness(sparkdf, src=\"src\", dst=\"dst\", distance_colname = \"distance\", cluster_id_colname=\"cluster_id\") Edge Betweeness \u201cThe edge betweeness graph metric counts the number of shortest paths between any two nodes from the cluster that use this edge. If there is more than one shortest path between a pair of nodes, each path is assigned equal weight such that the total weight of all of the paths is equal to unity.\" - ( Chatzoglou, Manassis, et al (2016) ). A high edge betweeness is cause for concern and so clusters may need to be clerically reviewed.","title":"edgebetweeness "},{"location":"splink_graph_guide.html#bridge_edges","text":"Returns any edges that are bridges. bridge_edges(sparkdf, src=\"src\", dst=\"dst\", distance_colname = \"distance\", cluster_id_colname=\"cluster_id\") Bridge Edges Bridge Edges are edges which join together two distinct clusters. I.e. any edges that if removed increase the total number of clusters. When bridges are present, this increases the edge betweeness of the cluster.","title":"bridge_edges "},{"location":"splink_graph_guide.html#resources","text":"For more information regarding the splink-graph package , please see MoJ github: https://github.com/moj-analytical-services/splink_graph . For a more in-depth look into graph data and graph theory , please see Network Science by Albert-L\u00e1szl\u00f3 Barab\u00e1si.","title":"Resources "},{"location":"splink_graph_guide.html#references","text":"Chatzoglou, C., Manassis, T., Gammon, S. and Swier, N., 2016. Use of Graph Databases to Improve the Management and Quality of Linked Data. https://espace.curtin.edu.au/bitstream/handle/20.500.11937/3205/199679_199679.pdf?sequence=2&isAllowed=y Croset, S., Rupp, J. and Romacker, M., 2016. Flexible data integration and curation using a graph-based approach. Bioinformatics, 32(6), pp.918-925. https://academic.oup.com/bioinformatics/article/32/6/918/1743746 Ellens, W. and Kooij, R.E., 2013. Graph measures and network robustness. arXiv preprint arXiv:1311.5064. https://arxiv.org/pdf/1311.5064 Noldus, R. and Van Mieghem, P., 2015. Assortativity in complex networks. Journal of Complex Networks, 3(4), pp.507-542. https://nas.ewi.tudelft.nl/people/Piet/papers/JCN2015AssortativitySurveyRogier.pdf Randall, S.M., Boyd, J.H., Ferrante, A.M., Bauer, J.K. and Semmens, J.B., 2014. Use of graph theory measures to identify errors in record linkage. Computer methods and programs in biomedicine, 115(2), pp.55-63. https://espace.curtin.edu.au/bitstream/handle/20.500.11937/3205/199679_199679.pdf?sequence=2&isAllowed=y","title":"References "},{"location":"assets/assets.html","text":"folder where images etc for the documentation are stored \u00b6","title":"Assets"},{"location":"assets/assets.html#folder-where-images-etc-for-the-documentation-are-stored","text":"","title":"folder where images etc for the documentation are stored"}]}